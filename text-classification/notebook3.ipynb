{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(news.data[0])\n",
    "print type(news.target)\n",
    "print type(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "18846\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print len(news.data)\n",
    "print len(news.target)\n",
    "print len(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "print news.target_names[news.target[0]]\n",
    "print news.data[0]\n",
    "print type(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    " \n",
    "    model.fit(X_train, y_train)\n",
    "    print \"Accuracy: %s\" % model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1\n",
    "1. No Lemmatization\n",
    "1. No Stop-Words\n",
    "1. Unigrams \n",
    "1. No min/max document frequency limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7554376657824934\n"
     ]
    }
   ],
   "source": [
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(input='content',encoding='utf-8', lowercase='true')),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial1,news.data,news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2\n",
    "1. No Lemmatization\n",
    "1. Yes Stop-Words\n",
    "1. Unigrams \n",
    "1. No min/max document frequency limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')       # For Stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7615384615384615\n"
     ]
    }
   ],
   "source": [
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(input='content',encoding='utf-8', lowercase='true', stop_words=stopwords)),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial2,news.data,news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 3\n",
    "1. Yes Lemmatization\n",
    "1. Yes Stop-Words\n",
    "1. Unigrams \n",
    "1. No min/max document frequency limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')           # For Tokenizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag         # POS Tagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank (pos_tag) tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word, pos=wn.NOUN):\n",
    "    return wnl.lemmatize(word,pos=pos)\n",
    "\n",
    "def tokenize(text):\n",
    "     return nltk.tokenize.word_tokenize(text)\n",
    "    \n",
    "def preprocess(text):\n",
    "    preprocessedtext = []\n",
    "    words = tokenize(text)\n",
    "    tagged_text = pos_tag(words)\n",
    "    for word, tag in tagged_text:\n",
    "        word = word.lower()\n",
    "        tag = penn_to_wn(tag)\n",
    "        if tag in (wn.NOUN,wn.ADJ,wn.VERB,wn.ADV):\n",
    "            root = lemmatize(word,tag)\n",
    "            preprocessedtext.append(root)\n",
    "    return preprocessedtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "stopwords.append(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'be', u'sure', u'bashers', u'pen', u'fan', u'be', u'pretty', u'confuse', u'lack', u'kind', u'post', u'recent', u'pen', u'massacre', u'devil', u'actually', u'be', u'bit', u'puzzled', u'too', u'bit', u'relieved', u'however', u'be', u'go', u'put', u'end', u'non-pittsburghers', u'relief', u'bit', u'praise', u'pen', u'man', u'be', u'kill', u'devil', u'bad', u'think', u'jagr', u'just', u'show', u'be', u'much', u'good', u'regular', u'season', u'stats', u'be', u'also', u'lot', u'fo', u'fun', u'watch', u'playoff', u'bowman', u'let', u'jagr', u'have', u'lot', u'fun', u'next', u'couple', u'game', u'pen', u'be', u'go', u'beat', u'pulp', u'jersey', u'anyway', u'be', u'very', u'disappointed', u'not', u'see', u'islander', u'lose', u'final', u'regular', u'season', u'game', u'pen', u'rule']\n"
     ]
    }
   ],
   "source": [
    "print preprocess(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7514588859416446\n"
     ]
    }
   ],
   "source": [
    "trial3 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(input='content',encoding='utf-8', lowercase='true', \n",
    "                                   tokenizer=preprocess, stop_words=stopwords)),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial3,news.data,news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 4\n",
    "1. Yes Lemmatization\n",
    "1. Yes Stop-Words\n",
    "1. Unigrams and Bigrams \n",
    "1. No min/max document frequency limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7538461538461538\n"
     ]
    }
   ],
   "source": [
    "trial4 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(input='content',encoding='utf-8', lowercase='true', \n",
    "                                   tokenizer=preprocess, stop_words=stopwords, ngram_range=(1,2))),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial4,news.data,news.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
