{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(news.data)\n",
    "print type(news.target)\n",
    "print type(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "18846\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print len(news.data)\n",
    "print len(news.target)\n",
    "print len(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "<type 'unicode'>\n"
     ]
    }
   ],
   "source": [
    "print news.target_names[news.target[0]]\n",
    "print news.data[0]\n",
    "print type(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train(model, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    " \n",
    "    model.fit(X_train, y_train)\n",
    "    print \"Accuracy: %s\" % model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1\n",
    "1. No Lemmatization\n",
    "1. No Tokenization\n",
    "1. No Stop-Words\n",
    "1. Unigrams \n",
    "1. No min/max document frequency limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7554376657824934\n"
     ]
    }
   ],
   "source": [
    "trial1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(input='content',encoding='utf-8', lowercase='true', \n",
    "                                   preprocessor=None, tokenizer=None, analyzer='word', stop_words=None, \n",
    "                                   ngram_range=(1,1), max_df=1.0, min_df=1, norm='l2')),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial1,news.data,news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2\n",
    "1. No Lemmatization\n",
    "1. No Tokenization\n",
    "1. Yes Stop-Words\n",
    "1. Unigrams \n",
    "1. No min/max document frequency limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')       # For Stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7615384615384615\n"
     ]
    }
   ],
   "source": [
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(input='content',encoding='utf-8', lowercase='true', \n",
    "                                   preprocessor=None, tokenizer=None, analyzer='word', stop_words=stopwords, \n",
    "                                   ngram_range=(1,1), max_df=1.0, min_df=1, norm='l2')),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial2,news.data,news.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
