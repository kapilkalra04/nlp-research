{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS:\n",
    "\n",
    "#### Pipeline - 1\n",
    "1. Tokenization\n",
    "1. Remove StopWords and Punctuation\n",
    "1. Stemming\n",
    "\n",
    "#### Pipeline - 2\n",
    "1. Tokenization\n",
    "1. POS Tagger\n",
    "1. Lemmatization\n",
    "\n",
    "***Remember to Deal With Everything in Lower Cases***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')           # For Tokenizing\n",
    "nltk.download('stopwords')       # For Stopwords\n",
    "nltk.download('wordnet')         # For Lemmatization\n",
    "nltk.download('averaged_perceptron_tagger') # For POS Tagging\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag         # POS Tagger\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\n",
    "    'Industrial Disease',\n",
    "'Private Investigations',\n",
    "'So Far Away',\n",
    "'Twisting by the Pool',\n",
    "'Skateaway',\n",
    "'Walk of Life',\n",
    "'Romeo and Juliet',\n",
    "'Tunnel of Love',\n",
    "'Money for Nothing',\n",
    "'Sultans of Swing',\n",
    "'Stairway To Heaven',\n",
    "'Kashmir',\n",
    "'Achilles Last Stand',\n",
    "'Whole Lotta Love',\n",
    "'Immigrant Song',\n",
    "'Black Dog',\n",
    "'When The Levee Breaks',\n",
    "'Since I\\'ve Been Lovin\\' You',\n",
    "'Since I\\'ve Been Loving You',\n",
    "'Over the Hills and Far Away',\n",
    "'Dazed and Confused' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "     return nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since I've Been Lovin' You\n",
      "['Since', 'I', \"'ve\", 'Been', 'Lovin', \"'\", 'You']\n"
     ]
    }
   ],
   "source": [
    "print input[17]\n",
    "print tokenize(input[17])             # there a \"(\\')\" being tokenized we will remove it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS TAGGER CONVERTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank (pos_tag) tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word, pos=wn.NOUN):\n",
    "    return wnl.lemmatize(word,pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twisting by the Pool\n",
      "['Twisting', 'by', 'the', 'Pool']\n",
      "Twisting\n",
      "by\n",
      "the\n",
      "Pool\n"
     ]
    }
   ],
   "source": [
    "print input[3]\n",
    "print tokenize(input[3])\n",
    "for w in tokenize(input[3]):\n",
    "    w.lower()\n",
    "    print lemmatize(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    stemmer = EnglishStemmer()\n",
    "    return stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twisting by the Pool\n",
      "['Twisting', 'by', 'the', 'Pool']\n",
      "twist\n",
      "by\n",
      "the\n",
      "pool\n"
     ]
    }
   ],
   "source": [
    "print input[3]\n",
    "print tokenize(input[3])\n",
    "for w in tokenize(input[3]):\n",
    "    w.lower()\n",
    "    print stem(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "print stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDEXED DATABASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in input:\n",
    "    words = tokenize(sentence)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and word not in string.punctuation:\n",
    "            root = stem(word)\n",
    "            if db.has_key(root):\n",
    "                db[root].append(sentence)\n",
    "            else :\n",
    "                db[root] = [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'achill': ['Achilles Last Stand'],\n",
       " u'away': ['So Far Away', 'Over the Hills and Far Away'],\n",
       " u'black': ['Black Dog'],\n",
       " u'break': ['When The Levee Breaks'],\n",
       " u'confus': ['Dazed and Confused'],\n",
       " u'daze': ['Dazed and Confused'],\n",
       " u'diseas': ['Industrial Disease'],\n",
       " u'dog': ['Black Dog'],\n",
       " u'far': ['So Far Away', 'Over the Hills and Far Away'],\n",
       " u'heaven': ['Stairway To Heaven'],\n",
       " u'hill': ['Over the Hills and Far Away'],\n",
       " u'immigr': ['Immigrant Song'],\n",
       " u'industri': ['Industrial Disease'],\n",
       " u'investig': ['Private Investigations'],\n",
       " u'juliet': ['Romeo and Juliet'],\n",
       " u'kashmir': ['Kashmir'],\n",
       " u'last': ['Achilles Last Stand'],\n",
       " u'leve': ['When The Levee Breaks'],\n",
       " u'life': ['Walk of Life'],\n",
       " u'lotta': ['Whole Lotta Love'],\n",
       " u'love': ['Tunnel of Love', 'Whole Lotta Love', \"Since I've Been Loving You\"],\n",
       " u'lovin': [\"Since I've Been Lovin' You\"],\n",
       " u'money': ['Money for Nothing'],\n",
       " u'noth': ['Money for Nothing'],\n",
       " u'pool': ['Twisting by the Pool'],\n",
       " u'privat': ['Private Investigations'],\n",
       " u'romeo': ['Romeo and Juliet'],\n",
       " u'sinc': [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"],\n",
       " u'skateaway': ['Skateaway'],\n",
       " u'song': ['Immigrant Song'],\n",
       " u'stairway': ['Stairway To Heaven'],\n",
       " u'stand': ['Achilles Last Stand'],\n",
       " u'sultan': ['Sultans of Swing'],\n",
       " u'swing': ['Sultans of Swing'],\n",
       " u'tunnel': ['Tunnel of Love'],\n",
       " u'twist': ['Twisting by the Pool'],\n",
       " u've': [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"],\n",
       " u'walk': ['Walk of Life'],\n",
       " u'whole': ['Whole Lotta Love']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in input:\n",
    "    words = tokenize(sentence)\n",
    "    tagged_sentence = pos_tag(words)\n",
    "    for word, tag in tagged_sentence:\n",
    "        word = word.lower()\n",
    "        tag = penn_to_wn(tag)\n",
    "        if tag in (wn.NOUN,wn.ADJ,wn.VERB,wn.ADV):\n",
    "            root = lemmatize(word,tag)\n",
    "            if db2.has_key(root):\n",
    "                db2[root].append(sentence)\n",
    "            else :\n",
    "                db2[root] = [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'ve\": [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"],\n",
       " 'achilles': ['Achilles Last Stand'],\n",
       " 'away': ['So Far Away', 'Over the Hills and Far Away'],\n",
       " 'been': [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"],\n",
       " 'black': ['Black Dog'],\n",
       " u'break': ['When The Levee Breaks'],\n",
       " u'confuse': ['Dazed and Confused'],\n",
       " u'daze': ['Dazed and Confused'],\n",
       " 'disease': ['Industrial Disease'],\n",
       " 'dog': ['Black Dog'],\n",
       " 'far': ['So Far Away', 'Over the Hills and Far Away'],\n",
       " 'heaven': ['Stairway To Heaven'],\n",
       " u'hill': ['Over the Hills and Far Away'],\n",
       " 'immigrant': ['Immigrant Song'],\n",
       " 'industrial': ['Industrial Disease'],\n",
       " u'investigation': ['Private Investigations'],\n",
       " 'juliet': ['Romeo and Juliet'],\n",
       " 'kashmir': ['Kashmir'],\n",
       " 'last': ['Achilles Last Stand'],\n",
       " 'levee': ['When The Levee Breaks'],\n",
       " 'life': ['Walk of Life'],\n",
       " 'lotta': ['Whole Lotta Love'],\n",
       " 'love': ['Tunnel of Love', 'Whole Lotta Love'],\n",
       " 'lovin': [\"Since I've Been Lovin' You\"],\n",
       " 'loving': [\"Since I've Been Loving You\"],\n",
       " 'money': ['Money for Nothing'],\n",
       " 'nothing': ['Money for Nothing'],\n",
       " 'pool': ['Twisting by the Pool'],\n",
       " 'private': ['Private Investigations'],\n",
       " 'romeo': ['Romeo and Juliet'],\n",
       " 'skateaway': ['Skateaway'],\n",
       " 'so': ['So Far Away'],\n",
       " 'song': ['Immigrant Song'],\n",
       " 'stairway': ['Stairway To Heaven'],\n",
       " 'stand': ['Achilles Last Stand'],\n",
       " u'sultan': ['Sultans of Swing'],\n",
       " 'swing': ['Sultans of Swing'],\n",
       " 'tunnel': ['Tunnel of Love'],\n",
       " u'twist': ['Twisting by the Pool'],\n",
       " 'walk': ['Walk of Life'],\n",
       " 'whole': ['Whole Lotta Love']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WE CAN OBSERVE THAT BY USING LEMMATIZATION WE PRESERVED THE MORPHOLOGY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
