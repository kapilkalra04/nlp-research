{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\n",
    "    'Industrial Disease',\n",
    "'Private Investigations',\n",
    "'So Far Away',\n",
    "'Twisting by the Pool',\n",
    "'Skateaway',\n",
    "'Walk of Life',\n",
    "'Romeo and Juliet',\n",
    "'Tunnel of Love',\n",
    "'Money for Nothing',\n",
    "'Sultans of Swing',\n",
    "'Stairway To Heaven',\n",
    "'Kashmir',\n",
    "'Achilles Last Stand',\n",
    "'Whole Lotta Love',\n",
    "'Immigrant Song',\n",
    "'Black Dog',\n",
    "'When The Levee Breaks',\n",
    "'Since I\\'ve Been Lovin\\' You',\n",
    "'Since I\\'ve Been Loving You',\n",
    "'Over the Hills and Far Away',\n",
    "'Dazed and Confused' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "     return nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole Lotta Love\n",
      "['Whole', 'Lotta', 'Love']\n"
     ]
    }
   ],
   "source": [
    "print input[13]\n",
    "print tokenize(input[13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
