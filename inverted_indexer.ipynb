{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')           # For Tokenizing\n",
    "nltk.download('stopwords')       # For Stopwords\n",
    "nltk.download('wordnet')         # For Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\n",
    "    'Industrial Disease',\n",
    "'Private Investigations',\n",
    "'So Far Away',\n",
    "'Twisting by the Pool',\n",
    "'Skateaway',\n",
    "'Walk of Life',\n",
    "'Romeo and Juliet',\n",
    "'Tunnel of Love',\n",
    "'Money for Nothing',\n",
    "'Sultans of Swing',\n",
    "'Stairway To Heaven',\n",
    "'Kashmir',\n",
    "'Achilles Last Stand',\n",
    "'Whole Lotta Love',\n",
    "'Immigrant Song',\n",
    "'Black Dog',\n",
    "'When The Levee Breaks',\n",
    "'Since I\\'ve Been Lovin\\' You',\n",
    "'Since I\\'ve Been Loving You',\n",
    "'Over the Hills and Far Away',\n",
    "'Dazed and Confused' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "     return nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since I've Been Lovin' You\n",
      "['Since', 'I', \"'ve\", 'Been', 'Lovin', \"'\", 'You']\n"
     ]
    }
   ],
   "source": [
    "print input[17]\n",
    "print tokenize(input[17])             # there a \"(\\')\" being tokenized wewill remove it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    return wnl.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole Lotta Love\n",
      "['Whole', 'Lotta', 'Love']\n",
      "Whole\n",
      "Lotta\n",
      "Love\n"
     ]
    }
   ],
   "source": [
    "print input[13]\n",
    "print tokenize(input[13])\n",
    "for w in tokenize(input[13]):\n",
    "    print lemmatize(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "print stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in input:\n",
    "    words = tokenize(sentence)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and word not in ['.',',','!',\"\\'\"] :\n",
    "            root = lemmatize(word)\n",
    "            if db.has_key(root):\n",
    "                db[root].append(sentence)\n",
    "            else :\n",
    "                db[root] = [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'industrial': ['Industrial Disease'], 'love': ['Tunnel of Love', 'Whole Lotta Love'], 'money': ['Money for Nothing'], 'confused': ['Dazed and Confused'], 'private': ['Private Investigations'], 'walk': ['Walk of Life'], 'dog': ['Black Dog'], 'juliet': ['Romeo and Juliet'], 'lotta': ['Whole Lotta Love'], 'away': ['So Far Away', 'Over the Hills and Far Away'], 'since': [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"], u'investigation': ['Private Investigations'], 'black': ['Black Dog'], u'hill': ['Over the Hills and Far Away'], 'loving': [\"Since I've Been Loving You\"], 'stairway': ['Stairway To Heaven'], 'skateaway': ['Skateaway'], 'life': ['Walk of Life'], 'song': ['Immigrant Song'], 'far': ['So Far Away', 'Over the Hills and Far Away'], 'dazed': ['Dazed and Confused'], 'romeo': ['Romeo and Juliet'], u'break': ['When The Levee Breaks'], 'nothing': ['Money for Nothing'], 'achilles': ['Achilles Last Stand'], 'immigrant': ['Immigrant Song'], 'pool': ['Twisting by the Pool'], 'heaven': ['Stairway To Heaven'], 'last': ['Achilles Last Stand'], u'sultan': ['Sultans of Swing'], 'tunnel': ['Tunnel of Love'], 'twisting': ['Twisting by the Pool'], 'kashmir': ['Kashmir'], 'disease': ['Industrial Disease'], 'levee': ['When The Levee Breaks'], \"'ve\": [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"], 'stand': ['Achilles Last Stand'], 'swing': ['Sultans of Swing'], 'lovin': [\"Since I've Been Lovin' You\"], 'whole': ['Whole Lotta Love']}\n"
     ]
    }
   ],
   "source": [
    "print db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industrial\n",
      "love\n",
      "money\n",
      "confused\n",
      "private\n",
      "walk\n",
      "dog\n",
      "juliet\n",
      "lotta\n",
      "away\n",
      "since\n",
      "investigation\n",
      "black\n",
      "hill\n",
      "loving\n",
      "stairway\n",
      "skateaway\n",
      "life\n",
      "song\n",
      "far\n",
      "dazed\n",
      "romeo\n",
      "break\n",
      "nothing\n",
      "achilles\n",
      "immigrant\n",
      "pool\n",
      "heaven\n",
      "last\n",
      "sultan\n",
      "tunnel\n",
      "twisting\n",
      "kashmir\n",
      "disease\n",
      "levee\n",
      "'ve\n",
      "stand\n",
      "swing\n",
      "lovin\n",
      "whole\n"
     ]
    }
   ],
   "source": [
    "for k in db.keys():\n",
    "    print k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
