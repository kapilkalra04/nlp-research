{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kapilkalra04\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')           # For Tokenizing\n",
    "nltk.download('stopwords')       # For Stopwords\n",
    "nltk.download('wordnet')         # For Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [\n",
    "    'Industrial Disease',\n",
    "'Private Investigations',\n",
    "'So Far Away',\n",
    "'Twisting by the Pool',\n",
    "'Skateaway',\n",
    "'Walk of Life',\n",
    "'Romeo and Juliet',\n",
    "'Tunnel of Love',\n",
    "'Money for Nothing',\n",
    "'Sultans of Swing',\n",
    "'Stairway To Heaven',\n",
    "'Kashmir',\n",
    "'Achilles Last Stand',\n",
    "'Whole Lotta Love',\n",
    "'Immigrant Song',\n",
    "'Black Dog',\n",
    "'When The Levee Breaks',\n",
    "'Since I\\'ve Been Lovin\\' You',\n",
    "'Since I\\'ve Been Loving You',\n",
    "'Over the Hills and Far Away',\n",
    "'Dazed and Confused' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "     return nltk.tokenize.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since I've Been Lovin' You\n",
      "['Since', 'I', \"'ve\", 'Been', 'Lovin', \"'\", 'You']\n"
     ]
    }
   ],
   "source": [
    "print input[17]\n",
    "print tokenize(input[17])             # there a \"(\\')\" being tokenized wewill remove it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    return wnl.lemmatize(word,pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twisting by the Pool\n",
      "['Twisting', 'by', 'the', 'Pool']\n",
      "Twisting\n",
      "by\n",
      "the\n",
      "Pool\n"
     ]
    }
   ],
   "source": [
    "print input[3]\n",
    "print tokenize(input[3])\n",
    "for w in tokenize(input[3]):\n",
    "    print lemmatize(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEMMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    stemmer = EnglishStemmer()\n",
    "    return stemmer.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twisting by the Pool\n",
      "['Twisting', 'by', 'the', 'Pool']\n",
      "twist\n",
      "by\n",
      "the\n",
      "pool\n"
     ]
    }
   ],
   "source": [
    "print input[3]\n",
    "print tokenize(input[3])\n",
    "for w in tokenize(input[3]):\n",
    "    print stem(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u\"you're\", u\"you've\", u\"you'll\", u\"you'd\", u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u\"she's\", u'her', u'hers', u'herself', u'it', u\"it's\", u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u\"that'll\", u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u\"don't\", u'should', u\"should've\", u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u\"aren't\", u'couldn', u\"couldn't\", u'didn', u\"didn't\", u'doesn', u\"doesn't\", u'hadn', u\"hadn't\", u'hasn', u\"hasn't\", u'haven', u\"haven't\", u'isn', u\"isn't\", u'ma', u'mightn', u\"mightn't\", u'mustn', u\"mustn't\", u'needn', u\"needn't\", u'shan', u\"shan't\", u'shouldn', u\"shouldn't\", u'wasn', u\"wasn't\", u'weren', u\"weren't\", u'won', u\"won't\", u'wouldn', u\"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('English')\n",
    "print stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in input:\n",
    "    words = tokenize(sentence)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and word not in ['.',',','!',\"\\'\"] :\n",
    "            root = lemmatize(word)\n",
    "            if db.has_key(root):\n",
    "                db[root].append(sentence)\n",
    "            else :\n",
    "                db[root] = [sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'industrial': ['Industrial Disease'], 'love': ['Tunnel of Love', 'Whole Lotta Love', \"Since I've Been Loving You\"], 'money': ['Money for Nothing'], u'confuse': ['Dazed and Confused'], 'private': ['Private Investigations'], 'walk': ['Walk of Life'], u'twist': ['Twisting by the Pool'], 'sultans': ['Sultans of Swing'], 'juliet': ['Romeo and Juliet'], 'lotta': ['Whole Lotta Love'], 'away': ['So Far Away', 'Over the Hills and Far Away'], 'song': ['Immigrant Song'], 'black': ['Black Dog'], u'hill': ['Over the Hills and Far Away'], u'daze': ['Dazed and Confused'], 'stairway': ['Stairway To Heaven'], 'skateaway': ['Skateaway'], 'life': ['Walk of Life'], 'investigations': ['Private Investigations'], 'far': ['So Far Away', 'Over the Hills and Far Away'], 'romeo': ['Romeo and Juliet'], u'break': ['When The Levee Breaks'], 'nothing': ['Money for Nothing'], 'achilles': ['Achilles Last Stand'], 'immigrant': ['Immigrant Song'], 'pool': ['Twisting by the Pool'], 'heaven': ['Stairway To Heaven'], 'last': ['Achilles Last Stand'], 'since': [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"], 'tunnel': ['Tunnel of Love'], 'dog': ['Black Dog'], 'kashmir': ['Kashmir'], 'disease': ['Industrial Disease'], 'levee': ['When The Levee Breaks'], \"'ve\": [\"Since I've Been Lovin' You\", \"Since I've Been Loving You\"], 'stand': ['Achilles Last Stand'], 'swing': ['Sultans of Swing'], 'lovin': [\"Since I've Been Lovin' You\"], 'whole': ['Whole Lotta Love']}\n"
     ]
    }
   ],
   "source": [
    "print db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "industrial\n",
      "love\n",
      "money\n",
      "confuse\n",
      "private\n",
      "walk\n",
      "twist\n",
      "sultans\n",
      "juliet\n",
      "lotta\n",
      "away\n",
      "song\n",
      "black\n",
      "hill\n",
      "daze\n",
      "stairway\n",
      "skateaway\n",
      "life\n",
      "investigations\n",
      "far\n",
      "romeo\n",
      "break\n",
      "nothing\n",
      "achilles\n",
      "immigrant\n",
      "pool\n",
      "heaven\n",
      "last\n",
      "since\n",
      "tunnel\n",
      "dog\n",
      "kashmir\n",
      "disease\n",
      "levee\n",
      "'ve\n",
      "stand\n",
      "swing\n",
      "lovin\n",
      "whole\n"
     ]
    }
   ],
   "source": [
    "for k in db.keys():\n",
    "    print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
